{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入相关依赖的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from os import listdir\n",
    "import codecs  # 字符转化模块，用于文本的编码和解码\n",
    "import jieba  # 中文分词库\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from itertools import chain  # 用于串联迭代对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建文本处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment2word(doc: str):\n",
    "    # 从 stop_list.txt 文件中提取停用词，存储为列表\n",
    "    stop_words = (\n",
    "        codecs.open(\"./ML/04/stop_list.txt\", \"r\", encoding=\"utf8\").read().splitlines()\n",
    "    )\n",
    "    doc = re.sub(r\"[\\t\\r\\n]\", \"\", doc)  # 去除邮件文本中的缩进，换行等\n",
    "    word_list = list(jieba.cut(doc.strip()))  # 用 jieba 进行分词\n",
    "    out_str = \"\"\n",
    "    for word in word_list:  # 删除邮件文本中的停用词\n",
    "        if word == \" \" or word == \"\":\n",
    "            continue\n",
    "        if word not in stop_words:\n",
    "            out_str += word.strip()\n",
    "            out_str += \" \"\n",
    "    segments = out_str.strip().split(sep=\" \")\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建文本读取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromDir(data_dir):\n",
    "    docLists = []\n",
    "    docLabels = [f for f in listdir(data_dir) if f.endswith(\".txt\")]  # 存储每一封邮件的名称\n",
    "    for doc in docLabels:\n",
    "        try:\n",
    "            filepath = data_dir + \"/\" + doc\n",
    "            # 对训练集的邮件进行文本处理\n",
    "            wordList = segment2word(codecs.open(filepath, \"r\", encoding=\"utf8\").read())\n",
    "            docLists.append(wordList) # 整合训练集的邮件处理后的结果\n",
    "        except:\n",
    "            print(\"handling file %s is failed\" % filepath)\n",
    "    return docLists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamDocList = getDataFromDir(\"./ML/04/email/spam\") # 对垃圾邮件进行文本处理\n",
    "hamDocList = getDataFromDir(\"./ML/04/email/ham\") # 对正常邮件进行文本处理\n",
    "fullDocList = spamDocList + hamDocList # 储存邮件的特征\n",
    "# 添加标签，垃圾邮件标记为1，正常邮件标记为0\n",
    "classList = array([1] * len(spamDocList) + [0] * len(hamDocList))\n",
    "frequncyDic = Counter(chain(*fullDocList)) # 生成词频映射字典\n",
    "topWords = [w[0] for w in frequncyDic.most_common(500)] # 选取前500个最频繁的热词\n",
    "vector = []\n",
    "\n",
    "for docList in fullDocList:\n",
    "    # 统计每封邮件中每个热词出现的频率\n",
    "    topwords_list = list(map(lambda x: docList.count(x), topWords))\n",
    "    vector.append(topwords_list)\n",
    "\n",
    "# 生成 vector 作为数据特征\n",
    "vector = array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB() # 选取多项式贝叶斯为训练模型\n",
    "model.fit(vector, classList) # vec 为特征，classList 为标签，巡礼那贝叶斯模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储每一封训练集邮件的名称\n",
    "dataList = []\n",
    "test_dir = \"./ML/04/email/spam\"\n",
    "docLabels = [f for f in listdir(test_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "# 模型推理\n",
    "for doc in docLabels:\n",
    "    try:\n",
    "        filepath = test_dir + \"/\" + doc\n",
    "        dataList = segment2word(codecs.open(filepath, \"r\", encoding=\"utf8\").read())\n",
    "    except:\n",
    "        print(\"handling file %s is failed\" % filepath)\n",
    "\n",
    "    # 统计测试集邮件中的热词的词频，提取特征\n",
    "    testVector = array(tuple(map(lambda x: dataList.count(x), topWords)))\n",
    "    testVector_reshape = testVector.reshape(1, -1)\n",
    "\n",
    "    # 特征传入模型进行推理\n",
    "    predicted_label = model.predict(testVector_reshape)\n",
    "    if predicted_label == 1:\n",
    "        print(\"%s is spam mail\" % doc)\n",
    "    else:\n",
    "        print(\"%s is NOT spam mail\" % doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
